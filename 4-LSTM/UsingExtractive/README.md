# Sequence-to-Sequence with Attention Model using Extractive Summarizer's Input

Here, we first generate an extractive summary of the input scientific article using Lex-Rank and feed this summarized text as input to the Sequence-To-Sequence model. Specifically, each word maps to a LSTM Unit in the encoder and the paper's abstract servers as the target sequence, present as the output of the decoder units during training.