# Sequence-to-Sequence with Attention Model using Paragraph Embeddings

Here, we give a compressed representation of the research text as input to the Sequence-To-Sequence Model. Specifically, we embed every section of a research paper using Para2Vec and the feed each para to a LSTM unit of the encoder. The extractive summary generated by Lex-Rank is the target sequence and present as decoder output during training.